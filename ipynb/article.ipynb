{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Water flows downstream, so does information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook, the data and the code can be downloaded [here](https://github.com/davidbrochart/hydro_forward_backward).\n",
    "\n",
    "In this notebook I will show that the measure of the streamflow at a point on a river can be transfered to any point on a river of the same hydrological basin, not only downstream but also upstream. It comes from the fact that a part of the water present in the streamflow of a river comes from other rivers that flow into it. This hierarchical relationship can be used by algorithms to backpropagate the streamflow signal. Possible applications can be found in situations where available data is not sufficient to unambiguously calibrate a model, for instance when sparse measurements of precipitation or streamflow are available, or where the quality of these measurements is not high enough, as it is the case for satellite-based estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A hydrological network in a basin is of hierarchical nature. As we can see in the figure below, lots of small streams flow into bigger ones, which get bigger and bigger as they collect more flows (while also being less and less numerous), and so on until only one river remains and reaches the basin's outlet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"../data/hydrosheds_amazon_large.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to represent the structure of a hydrological network, we get a tree where the trunc is the main river at the outlet of the basin, and the leaves are the streams from upland areas (see figure below). This hierarchical nature, where a node's streamflow depends on the streamflow of its children (not only in space but also in time), has some interesting characteristics that the [forward-backward algorithm](https://en.wikipedia.org/wiki/Forward%E2%80%93backward_algorithm) can take advante of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"../data/stream-order.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will limit the scope of this study to the simplest form of network - a linear one, where we have only one river that flows through several subbasins. In the figure below, we denote these subbasins by *b<sub>n</sub>*, while the basins that include the upstream subbasins are called *B<sub>n</sub>* and defined by:\n",
    "\n",
    "*B<sub>0</sub> = {b<sub>0</sub>}*\n",
    "\n",
    "*B<sub>1</sub> = {b<sub>0</sub>, b<sub>1</sub>}*\n",
    "\n",
    "*B<sub>2</sub> = {b<sub>0</sub>, b<sub>1</sub>, b<sub>2</sub>}*\n",
    "\n",
    "*B<sub>3</sub> = {b<sub>0</sub>, b<sub>1</sub>, b<sub>2</sub>, b<sub>3</sub>}*\n",
    "\n",
    "*B<sub>4</sub> = {b<sub>0</sub>, b<sub>1</sub>, b<sub>2</sub>, b<sub>3</sub>, b<sub>4</sub>}*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.sankey import Sankey\n",
    "fig = plt.figure(figsize=(16.9, 5))\n",
    "ax = fig.add_subplot(1, 1, 1, xticks=[], yticks=[])\n",
    "sankey = Sankey(ax=ax, unit=None)\n",
    "sankey.add(flows=[1, -1], trunklength=2, patchlabel='$b_0$')\n",
    "ws_nb = 5\n",
    "for i in range(1, ws_nb):\n",
    "    sankey.add(flows=[1, -1], trunklength=2, patchlabel=f'$b_{i}$', prior=i-1, connect=(1, 0))\n",
    "sankey.finish()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "import sys\n",
    "sys.path.append('../py')\n",
    "from models import gr4j, delay\n",
    "from misc import get_kde, uniform_density, lnprob_from_density\n",
    "import mcmc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first load some precipitation data. It doesn't matter very much where it comes from, it just looks like a plausible precipitation time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.load('../data/p.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take a constant potential evapotranspiration, even though this is not very plausible, but it is not very important here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame()\n",
    "df['p'] = p\n",
    "df['e'] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to generate synthetic streamflow data, we choose quite an arbitrary hydrological model, and feed it with the precipitation and potential evapotranspiration data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = gr4j([1000, 0, 100, 10])\n",
    "df['q_true_0'] = g.run([df['p'].values, df['e'].values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is for our basin *B<sub>0</sub>*. Now we want the stream at its outlet to flow into another subbasin, which itself flows into another subbasin, etc. We will suppose that all subbasins have the same area, and that the time it takes for the water to flow from the outlet of a subbasin to the outlet of the next one is the same for every subbasin, and can be described as a simple delay in the streamflow signal. The precipitation and potential evapotranspiration are uniform across all subbasins. From this we can compute the streamflow at the outlet of a basin recursively from the streamflow at the outlet of its upstream basin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay_days = 1\n",
    "d = delay(delay_days)\n",
    "for i in range(1, ws_nb):\n",
    "    area_head = i\n",
    "    area_tail = 1\n",
    "    df[f'q_true_{i}'] = (d.run(df[f'q_true_{i-1}'].values) * area_head + df['q_true_0'].values * area_tail) / (area_head + area_tail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to remove information from our synthetic streamflows. This can be done by inserting missing values and by adding noise. It is also similar to the type of data we can get from satellite estimates. The noise is chosen to be Gaussian around the true value, and the remaining data is spread in time accross the subbasins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsample = 30 # keep one data every 30 days for each subbasin\n",
    "offset = downsample // ws_nb\n",
    "for i in range(ws_nb):\n",
    "    q_min = np.nanmin(df[f'q_true_{i}'])\n",
    "    q_max = np.nanmax(df[f'q_true_{i}'])\n",
    "    # noise is gaussian around true value\n",
    "    std = (q_max - q_min) / 10\n",
    "    q_noised = np.random.normal(df[f'q_true_{i}'], std, df['p'].values.size)\n",
    "    q_noised = np.where(q_noised < 0, 0, q_noised)\n",
    "    q = np.ones_like(q_noised) * np.nan\n",
    "    q[offset*i::downsample] = q_noised[offset*i::downsample]\n",
    "    df[f'q{i}'] = q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below shows that the information we have about the streamflow is spread accross the subbasins, not only in space but also in time. The remaining of this study presents the forward-backward algorithm, which is well suited for taking advantage of such distributed information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = {f'q{i}':f'$Q_{i}$' for i in range(ws_nb)}\n",
    "columns.update({'e': '$E$', 'p': '$P$'})\n",
    "df[['p', 'e']].tail(2*360).rename(columns=columns).plot(figsize=(16.7, 5))\n",
    "for i in range(ws_nb):\n",
    "    plt.scatter(df.tail(2*360).index, df[f'q{i}'].tail(2*360).rename(columns=columns), label=f'$Q_{i}$')\n",
    "plt.legend()\n",
    "plt.title('Spread streamflow information accross subbasins')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to pass the information forward (i.e. downstream), just like water flows. We start by calibrating *B<sub>0</sub>* as we usually do, using the measurements (*P, E, Q<sub>0</sub>*) over it. This gives us some information about the model parameters, but they might not be fully known at this point because of the poor measurements available. The idea is then to use a model of *B<sub>1</sub>* that makes use of *B<sub>0</sub>* and *b<sub>1</sub>*'s models, and to calibrate this dual model with the measurements over *B<sub>1</sub>*. Note that the dual model consists of twice as many parameters as a single model (plus one parameter for the delay), which could be an issue if we didn't have any prior information about these parameters. But it is not the case: the calibration of *B<sub>0</sub>*'s model helped us know its parameters better. When this information is used during the calibration of *B<sub>1</sub>*'s model, it constraints the possible values of the parameters of *b<sub>1</sub>*'s model. So information has passed from *B<sub>0</sub>* to *b<sub>1</sub>*, and if we repeat this process for *B<sub>2</sub>*, and so on until the outlet, we effectively make the information flow downstream.\n",
    "\n",
    "Note that the basic mechanism in which information passes between two basins is the calibration of a dual model, i.e. a model that splits a basin in two submodels. It is through the calibration of this dual model that information can be transfered between the parameters of the two submodels, and consequently between the streamflows simulated by the submodels.\n",
    "\n",
    "Let's first get the posterior probability distribution for the model parameters of *B<sub>0</sub>*, given a streamflow *Q<sub>0</sub>*. We take a uniform distribution for the prior of these parameters, because we don't know anything about them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lnprob(peq, warmup):\n",
    "    # a closure to have peq and warmup already known in lnprob\n",
    "    def lnprob(x):\n",
    "        q_obs = peq.q.values\n",
    "        g = gr4j(x)\n",
    "        q_sim = g.run([peq.p.values, peq.e.values])\n",
    "        # remove warm-up data and missing values\n",
    "        df = DataFrame({'q_sim': q_sim, 'q_obs': q_obs})[warmup:].dropna()\n",
    "        # standard deviation = confidence we have in the observed data\n",
    "        std = (np.nanmax(q_obs) - np.nanmin(q_obs)) / 10\n",
    "        # error on observed streamflow is a gaussian with standard deviation std\n",
    "        return -np.sum(np.square(df.q_sim.values - df.q_obs.values)) / (2 * std * std) - np.log(np.sqrt(2 * np.pi * std * std))\n",
    "    return lnprob\n",
    "\n",
    "peq = df[['p', 'e']]\n",
    "peq['q'] = df['q0']\n",
    "warmup = 365 * 2 # two years for the warmup of the model\n",
    "lnprob = get_lnprob(peq, warmup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = [1000, 0, 100, 10] # initial parameter values\n",
    "sampler = mcmc.Sampler(x0, lnprob)\n",
    "sampler.run(100) # discard MCMC burn-in samples\n",
    "samples = sampler.run(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below shows the posterior probability distribution of *B<sub>0</sub>*'s model parameters (using Kernel Density Estimation). We can see that they are more or less centered on their true value, with some uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dist(dist, x0):\n",
    "    x_nb = len(dist)\n",
    "    f, ax = plt.subplots(1, x_nb)\n",
    "    f.set_figwidth(x_nb * 4)\n",
    "    for i in range(x_nb):\n",
    "        x, y = dist[i]\n",
    "        ax[i].plot(x, y)\n",
    "        ax[i].axvline(x0[i], color='r', alpha=0.3)\n",
    "        ax[i].set_title(f'$X_{i+1}$')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_b0 = [get_kde(samples[:, i]) for i in range(4)]\n",
    "plot_dist(x_b0, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For *B<sub>1</sub>*, with discharge *Q<sub>1</sub>*, we need a model that consists of the models of *B<sub>0</sub>* (with its propagation delay) and *b<sub>1</sub>*. Thus the *lnprob* function has to be a little more complex. Also, it has to take into account the prior information on the parameters of *B<sub>0</sub>*'s model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lnprob(peq, warmup, lnprob_prior):\n",
    "    def lnprob(x):\n",
    "        # prior\n",
    "        lnp = 0\n",
    "        for i, v in enumerate(x):\n",
    "            lnp += lnprob_prior[i](v)\n",
    "        if not np.isfinite(lnp):\n",
    "            return -np.inf, np.ones_like(peq.p.values) * np.inf\n",
    "\n",
    "        q_obs = peq.q.values\n",
    "        std = (np.nanmax(q_obs) - np.nanmin(q_obs)) / 10\n",
    "        x_head = x[:5]\n",
    "        g_head = gr4j(x_head)\n",
    "        x_tail = x[5:]\n",
    "        g_tail = gr4j(x_tail)\n",
    "        q_sim = (g_head.run([peq.p.values, peq.e.values]) + g_tail.run([peq.p.values, peq.e.values])) / 2\n",
    "        df = DataFrame({'q_sim': q_sim, 'q_obs': q_obs})[warmup:].dropna()\n",
    "        return lnp - np.sum(np.square(df.q_sim.values - df.q_obs.values)) / (2 * std * std) - np.log(np.sqrt(2 * np.pi * std * std)), q_sim\n",
    "    return lnprob\n",
    "\n",
    "peq = df[['p', 'e']]\n",
    "peq['q'] = df['q1']\n",
    "# prior probability distribution is uniform for subbasin b1\n",
    "x_b1 = [uniform_density(1e-3, 1e4), uniform_density(-1, 1), uniform_density(1e-3, 1e3), uniform_density(1e-3, 1e3)]\n",
    "d_prior = uniform_density(0, 1e3)\n",
    "x_prior = x_b0 + [d_prior] + x_b1\n",
    "lnprob_prior = [lnprob_from_density(p) for p in x_prior]\n",
    "lnprob = get_lnprob(peq, warmup, lnprob_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = [1000, 0, 100, 10] + [1] + [1000, 0, 100, 10]\n",
    "sampler = mcmc.Sampler(x0, lnprob)\n",
    "sampler.run(100)\n",
    "samples, q_sim = sampler.run(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the figure below, where the posterior probability distribution of *B<sub>0</sub>*'s model parameters is shown before (lighter blue) and after (darker blue) the update, we can see that our knowledge of the parameters has changed, based on the information present in *Q<sub>1</sub>*. Whether the parameters have converged towards their true value or not depends on how much information was added by the new observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_nb = 5\n",
    "f, ax = plt.subplots(1, x_nb)\n",
    "f.set_figwidth(x_nb * 3.2)\n",
    "for i in range(x_nb):\n",
    "    x, y = get_kde(samples[:, i])\n",
    "    if i < 4:\n",
    "        p = ax[i].plot(*x_b0[i], alpha=0.1)\n",
    "    ax[i].plot(x, y, color=p[0].get_color())\n",
    "    ax[i].axvline(x0[i], color='r', alpha=0.3)\n",
    "    ax[i].set_title(f'$X_{i+1}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below shows the posterior probability distribution for the parameters of *b<sub>1</sub>*'s model. Information from *B<sub>0</sub>*'s model parameters was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_b1 = [get_kde(samples[:, i]) for i in range(5, 9)]\n",
    "plot_dist(x_b1, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we want to continue this process downstream, we could just create a model which includes the models of all its upstream basins. But that would not scale, first from a computational point of view, because the MCMC algorithm needs to run *(N models) * (M parameters)* for each sample, and then from a problem complexity point of view, because each time we add a parameter we add a dimension to the space that the MCMC algorithm has to explore. Even if the prior information we have on the parameters limits this space, it is always very important to limit the number of parameters for the algorithm to converve.\n",
    "\n",
    "To prevent the problem complexity from growing, we can reduce our dual model to a single model. Indeed, we should now have a dual model that can produce (i.e. simulate) streamflow data with a better quality than the measured streamflow, since we also used the information in the streamflow of its upstream basin. All we need to do is to get the posterior probability distribution for the parameters of this single model using the simulated streamflow from the dual model. Fortunately, we already computed the streamflow of the dual model previously when we ran the MCMC algorithm. From this ensemble of streamflow time series, we get the mean and the standard deviation at each time step (considering that the distribution of the simulated streamflow is a Gaussian), as shown in the figure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_sim = np.array(q_sim)\n",
    "q_sim[q_sim==np.inf] = np.nan\n",
    "q_mean = np.nanmean(q_sim, axis=0)\n",
    "q_std = np.nanstd(q_sim, axis=0)\n",
    "q_std = np.where(q_std==0, np.min(q_std[np.nonzero(q_std)]), q_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_series(q_mean, q_std, q_true, q_meas, title):\n",
    "    q_low = q_mean - q_std\n",
    "    q_low = np.where(q_low < 0, 0, q_low)\n",
    "    q_high = q_mean + q_std\n",
    "    plt.figure(figsize=(16.6, 5))\n",
    "    plt.plot(q_mean, label='Simulated streamfllow')\n",
    "    plt.fill_between(np.arange(q_mean.size), q_low, q_high, color='r', alpha=0.2, label='Simulation uncertainty')\n",
    "    plt.plot(q_true, color='b', linestyle='dashed', alpha=0.3, label='True streamflow')\n",
    "    plt.scatter(np.arange(q_mean.size), q_meas, label='Measured streamflow')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = 2 * 365 # only plot two last years\n",
    "q_mean_plot = q_mean[-days:]\n",
    "q_std_plot = q_std[-days:] * 3 # show uncertainty between +/- 3 standard deviations\n",
    "plot_series(q_mean_plot, q_std_plot, df[f'q_true_1'].values[-days:], df[f'q1'].values[-days:], 'Simulated streamflow of $B_1$ (dual model) using measured streamflows $Q_0$ and $Q_1$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lnprob(peq, warmup):\n",
    "    def lnprob(x):\n",
    "        q_obs = peq.q_mean.values\n",
    "        g = gr4j(x)\n",
    "        q_sim = g.run([peq.p.values, peq.e.values])\n",
    "        q_std = peq.q_std.values\n",
    "        df = DataFrame({'q_sim': q_sim, 'q_obs': q_obs, 'q_std': q_std})[warmup:].dropna()\n",
    "        std2 = df.q_std.values * df.q_std.values\n",
    "        return -np.sum(np.square(df.q_sim.values - df.q_obs.values) / (2 * std2) - np.log(np.sqrt(2 * np.pi * std2))), q_sim\n",
    "    return lnprob\n",
    "\n",
    "peq = df[['p', 'e']]\n",
    "peq['q_mean'] = q_mean\n",
    "peq['q_std'] = q_std\n",
    "lnprob = get_lnprob(peq, warmup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = [1000, 0, 100, 10]\n",
    "sampler = mcmc.Sampler(x0, lnprob)\n",
    "sampler.run(100)\n",
    "samples, q_sim = sampler.run(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_sim = np.array(q_sim)\n",
    "q_sim[q_sim==np.inf] = np.nan\n",
    "q_mean = np.nanmean(q_sim, axis=0)\n",
    "q_std = np.nanstd(q_sim, axis=0)\n",
    "q_std = np.where(q_std==0, np.min(q_std[np.nonzero(q_std)]), q_std)\n",
    "\n",
    "q_mean_plot = q_mean[-days:]\n",
    "q_std_plot = q_std[-days:] * 3\n",
    "plot_series(q_mean_plot, q_std_plot, df[f'q_true_1'].values[-days:], df[f'q1'].values[-days:], 'Simulated streamflow of $B_1$ (single model) using measured streamflows $Q_0$ and $Q_1$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we are back in the situation we started in: we can create a dual model that consists of the models of *B<sub>1</sub>* and *b<sub>2</sub>*, etc. The following is the full algorithm which corresponds to the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lnprob(peq, warmup, lnprob_prior, area_head, area_tail):\n",
    "    def lnprob(x):\n",
    "        lnp = 0\n",
    "        for i, v in enumerate(x):\n",
    "            lnp += lnprob_prior[i](v)\n",
    "        if not np.isfinite(lnp):\n",
    "            return -np.inf, np.ones_like(peq.p.values) * np.inf\n",
    "        q_obs = peq.q_obs.values\n",
    "        q_std = peq.q_std.values\n",
    "        x_head = x[:5]\n",
    "        g_head = gr4j(x_head)\n",
    "        if area_tail > 0:\n",
    "            x_tail = x[5:]\n",
    "            g_tail = gr4j(x_tail)\n",
    "            q_tail = g_tail.run([peq.p.values, peq.e.values])\n",
    "        else:\n",
    "            q_tail = 0\n",
    "        q_sim = (g_head.run([peq.p.values, peq.e.values]) * area_head + q_tail * area_tail) / (area_head + area_tail)\n",
    "        df = DataFrame({'q_sim': q_sim, 'q_obs': q_obs, 'q_std': q_std})[warmup:].dropna()\n",
    "        std2 = df.q_std.values * df.q_std.values\n",
    "        return lnp - np.sum(np.square(df.q_sim.values - df.q_obs.values) / (2 * std2) - np.log(np.sqrt(2 * np.pi * std2))), q_sim\n",
    "    return lnprob\n",
    "\n",
    "q_ensemble = {}\n",
    "for ws_i in range(ws_nb):\n",
    "    peq = df[['p', 'e']]\n",
    "    peq['q_obs'] = df[f'q{ws_i}']\n",
    "    peq['q_std'] = (np.nanmax(peq.q_obs) - np.nanmin(peq.q_obs)) / 10\n",
    "    warmup = 365 * 2\n",
    "    if ws_i == 0:\n",
    "        area_head = 1\n",
    "        area_tail = 0\n",
    "        # prior probability distribution is uniform for head basin\n",
    "        x_head = [uniform_density(1e-6, 1e6), uniform_density(-1e6, 1e6), uniform_density(1e-6, 1e6), uniform_density(1e-6, 1e6)]\n",
    "        x0 = [1000, 0, 100, 10]\n",
    "    x_prior = x_head\n",
    "    if ws_i > 0:\n",
    "        area_head = ws_i\n",
    "        area_tail = 1\n",
    "        x0 = [xy[0][np.argmax(xy[1])] for xy in x_head]\n",
    "        x0 += [10] + [1000, 0, 100, 10]\n",
    "        # prior probability distribution is uniform for tail basin\n",
    "        d_prior = uniform_density(0, 1e2)\n",
    "        x_tail = [uniform_density(1e-3, 1e4), uniform_density(-1, 1), uniform_density(1e-3, 1e3), uniform_density(1e-3, 1e2)]\n",
    "        x_prior += [d_prior] + x_tail\n",
    "    lnprob_prior = [lnprob_from_density(p) for p in x_prior]\n",
    "    lnprob = get_lnprob(peq, warmup, lnprob_prior, area_head=area_head, area_tail=area_tail)\n",
    "    # run MCMC\n",
    "    sampler = mcmc.Sampler(x0, lnprob)\n",
    "    sampler.run(100) # discard burn-in samples\n",
    "    samples, q_sim = sampler.run(1000)\n",
    "    # get simulated streamflow and uncertainty\n",
    "    q_sim = np.array(q_sim)\n",
    "    q_sim[q_sim==np.inf] = np.nan\n",
    "    q_mean = np.nanmean(q_sim, axis=0)\n",
    "    q_std = np.nanstd(q_sim, axis=0)\n",
    "    q_std = np.where(q_std==0, np.min(q_std[np.nonzero(q_std)]), q_std)\n",
    "    df[f'q_fmean{ws_i}'] = q_mean\n",
    "    df[f'q_fstd{ws_i}'] = q_std\n",
    "    q_ensemble[f'f{ws_i}'] = q_sim\n",
    "    # plot updated streamflow\n",
    "    q_mean_plot = q_mean[-days:]\n",
    "    q_std_plot = q_std[-days:] * 3\n",
    "    plot_series(q_mean_plot, q_std_plot, df[f'q_true_{ws_i}'].values[-days:], df[f'q{ws_i}'].values[-days:], f'Updated streamflow at the outlet of $B_{ws_i}$')\n",
    "    if (ws_i > 0) and (ws_i < ws_nb - 1):\n",
    "        # reduce dual model to single model\n",
    "        peq = df[['p', 'e']]\n",
    "        peq['q_obs'] = q_mean\n",
    "        peq['q_std'] = q_std\n",
    "        warmup = 365 * 2\n",
    "        x_prior = [uniform_density(1e-6, 1e6), uniform_density(-1e6, 1e6), uniform_density(1e-6, 1e6), uniform_density(1e-6, 1e6)]\n",
    "        lnprob_prior = [lnprob_from_density(p) for p in x_prior]\n",
    "        lnprob = get_lnprob(peq, warmup, lnprob_prior, 1, 0)\n",
    "        x0 = [1000, 0, 100, 10]\n",
    "        sampler = mcmc.Sampler(x0, lnprob)\n",
    "        sampler.run(100)\n",
    "        samples, q_sim = sampler.run(1000)\n",
    "    x_nb = 4\n",
    "    x_head = [get_kde(samples[:, i]) for i in range(x_nb)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing the information forward results in the downstream basins benefiting from more information than the upstream basins. So for instance *Q<sup>4</sup>* is better known than *Q<sub>0</sub>* after the update. This doesn't have to be so, because we can feel that an information on a downstream basin can also constrain an upstream basin.\n",
    "\n",
    "The second step is to pass the information backward (i.e. upstream). This is less intuitive since water doesn't flow upstream, but from the information point of view it works both ways: when we calibrate *b<sub>1</sub>*'s model using *B<sub>0</sub>*'s model, we can say that the information in *Q<sub>0</sub>* constraints the possible values of *Q<sub>1</sub>* further (information passes forward), or we can equivalently say that the information in *Q<sub>1</sub>* constraints the possible values of *Q<sub>0</sub>* further (information passes backward). Now if instead of starting from the two first basins we start from the two last ones, and repeat this process by going upstream, we effectively make the information flow backward. The following is the full algorithm corresponding to the backward step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lnprob(peq, warmup, lnprob_prior, area_head, area_tail):\n",
    "    def lnprob(x):\n",
    "        lnp = 0\n",
    "        for i, v in enumerate(x):\n",
    "            lnp += lnprob_prior[i](v)\n",
    "        if not np.isfinite(lnp):\n",
    "            return -np.inf, np.ones_like(peq.p.values) * np.inf\n",
    "        q_obs = peq.q_obs.values\n",
    "        q_std = peq.q_std.values\n",
    "        x_head = x[:5]\n",
    "        g_head = gr4j(x_head)\n",
    "        if area_tail > 0:\n",
    "            x_tail = x[5:]\n",
    "            g_tail = gr4j(x_tail)\n",
    "            q_tail = g_tail.run([peq.p.values, peq.e.values])\n",
    "        else:\n",
    "            q_tail = 0\n",
    "        q_head = g_head.run([peq.p.values, peq.e.values])\n",
    "        q_sim = (q_head * area_head + q_tail * area_tail) / (area_head + area_tail)\n",
    "        df = DataFrame({'q_sim': q_sim, 'q_obs': q_obs, 'q_std': q_std})[warmup:].dropna()\n",
    "        std2 = df.q_std.values * df.q_std.values\n",
    "        return lnp - np.sum(np.square(df.q_sim.values - df.q_obs.values) / (2 * std2) - np.log(np.sqrt(2 * np.pi * std2))), q_head\n",
    "    return lnprob\n",
    "\n",
    "for ws_i in range(ws_nb-1, 0, -1):\n",
    "    if ws_i == ws_nb - 1:\n",
    "        nb = 2\n",
    "    else:\n",
    "        nb = 1\n",
    "    for n in range(nb-1, -1, -1):\n",
    "        peq = df[['p', 'e']]\n",
    "        peq['q_obs'] = df[f'q{ws_i - 1 + n}']\n",
    "        peq['q_std'] = (np.nanmax(peq.q_obs) - np.nanmin(peq.q_obs)) / 10\n",
    "        warmup = 365 * 2\n",
    "        x_prior = [uniform_density(1e-6, 1e6), uniform_density(-1e6, 1e6), uniform_density(1e-6, 1e6), uniform_density(1e-6, 1e6)]\n",
    "        x0 = [1000, 0, 100, 10]\n",
    "        lnprob_prior = [lnprob_from_density(p) for p in x_prior]\n",
    "        lnprob = get_lnprob(peq, warmup, lnprob_prior, 1, 0)\n",
    "        # run MCMC\n",
    "        sampler = mcmc.Sampler(x0, lnprob)\n",
    "        sampler.run(100) # discard burn-in samples\n",
    "        samples, q_sim = sampler.run(1000)\n",
    "        q_sim = np.array(q_sim)\n",
    "        q_sim[q_sim==np.inf] = np.nan\n",
    "        q_mean = np.nanmean(q_sim, axis=0)\n",
    "        q_std = np.nanstd(q_sim, axis=0)\n",
    "        q_std = np.where(q_std==0, np.min(q_std[np.nonzero(q_std)]), q_std)\n",
    "        if n == 1:\n",
    "            # last basin: no update\n",
    "            df[f'q_bmean{ws_nb - 1}'] = q_mean\n",
    "            df[f'q_bstd{ws_nb - 1}'] = q_std\n",
    "            q_ensemble[f'b{ws_nb - 1}'] = q_sim\n",
    "        else:\n",
    "            x_head = [get_kde(samples[:, i]) for i in range(4)]\n",
    "\n",
    "    peq = df[['p', 'e']]\n",
    "    if ws_i == ws_nb - 1:\n",
    "        peq['q_obs'] = df[f'q{ws_i}']\n",
    "        peq['q_std'] = (np.nanmax(peq.q_obs) - np.nanmin(peq.q_obs)) / 10\n",
    "    else:\n",
    "        peq['q_obs'] = q_mean\n",
    "        peq['q_std'] = q_std\n",
    "    warmup = 365 * 2\n",
    "    x_prior = x_head\n",
    "    area_head = ws_i\n",
    "    area_tail = 1\n",
    "    x0 = [xy[0][np.argmax(xy[1])] for xy in x_head]\n",
    "    x0 += [10] + [1000, 0, 100, 10]\n",
    "    # prior probability distribution is uniform for tail basin\n",
    "    d_prior = uniform_density(0, 1e2)\n",
    "    x_tail = [uniform_density(1e-3, 1e4), uniform_density(-1, 1), uniform_density(1e-3, 1e3), uniform_density(1e-3, 1e2)]\n",
    "    x_prior += [d_prior] + x_tail\n",
    "    lnprob_prior = [lnprob_from_density(p) for p in x_prior]\n",
    "    lnprob = get_lnprob(peq, warmup, lnprob_prior, area_head=area_head, area_tail=area_tail)\n",
    "    # run MCMC\n",
    "    sampler = mcmc.Sampler(x0, lnprob)\n",
    "    sampler.run(100) # discard burn-in samples\n",
    "    samples, q_sim = sampler.run(1000)\n",
    "    # get simulated streamflow and uncertainty\n",
    "    q_sim = np.array(q_sim)\n",
    "    q_sim[q_sim==np.inf] = np.nan\n",
    "    q_mean = np.nanmean(q_sim, axis=0)\n",
    "    q_std = np.nanstd(q_sim, axis=0)\n",
    "    q_std = np.where(q_std==0, np.min(q_std[np.nonzero(q_std)]), q_std)\n",
    "    df[f'q_bmean{ws_i-1}'] = q_mean\n",
    "    df[f'q_bstd{ws_i-1}'] = q_std\n",
    "    q_ensemble[f'b{ws_i - 1}'] = q_sim\n",
    "    # plot updated streamflow\n",
    "    if ws_i == ws_nb - 1:\n",
    "        nb = 2\n",
    "    else:\n",
    "        nb = 1\n",
    "    for n in range(nb-1, -1, -1):\n",
    "        q_mean_plot = df[f'q_bmean{ws_i-1+n}'].values[-days:]\n",
    "        q_std_plot = df[f'q_bstd{ws_i-1+n}'].values[-days:] * 3\n",
    "        plot_series(q_mean_plot, q_std_plot, df[f'q_true_{ws_i-1+n}'].values[-days:], df[f'q{ws_i-1+n}'].values[-days:], f'Updated streamflow at the outlet of $B_{ws_i-1+n}$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to merge the forward and the backward passes, since they are complementary in their use of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ws_i in range(ws_nb):\n",
    "    q_sim = np.vstack((q_ensemble[f'f{ws_i}'], q_ensemble[f'b{ws_i}']))\n",
    "    q_mean = np.nanmean(q_sim, axis=0)\n",
    "    q_std = np.nanstd(q_sim, axis=0)\n",
    "    q_std = np.where(q_std==0, np.min(q_std[np.nonzero(q_std)]), q_std)\n",
    "    q_mean_plot = q_mean[-days:]\n",
    "    q_std_plot = q_std[-days:] * 3\n",
    "    plot_series(q_mean_plot, q_std_plot, df[f'q_true_{ws_i}'].values[-days:], df[f'q{ws_i}'].values[-days:], f'Updated streamflow at the outlet of $B_{ws_i}$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook showed how the forward-backward algorithm can be applied to hydrology in the context of distributed information, e.g. when sparse streamflow measurements are spread over the hydrological network, and/or when these measurements are of low quality. The power of the bayesian framework can be leveraged to take advantage of this situation and pass the information downstream (forward) and upstream (backward), resulting in an efficient use of the overall information. Satellite data is well suited to this kind of processing due to its spatial nature and poor precision. In particular, satellite-based streamflow estimates derived from water level is a good candidate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
